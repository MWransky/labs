{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "\n",
    "In this lab we'll create a neural network from scratch. In the process we'll learn about the algorithm that makes it all possible, **backpropagation**. In the process we'll create a library **miniflow** and use it to train a neural network on [MNIST](http://yann.lecun.com/exdb/mnist/), a dataset of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Address the following\n",
    "\n",
    "* Why am I creating a neural network from scratch?\n",
    "* Can you give me some motivation?\n",
    "* Why not just use a library?\n",
    "* What is your goal for me at the end of this? What should I be able to do that I wasnâ€™t able to do before?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions, Backpropagation and the Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of a neural network as a composition of functions with a **loss** (number) as the output. This number can also be thought of as an *error term*, the larger the value the larger our error. Our goal is to get the error as close to 0 as possible.\n",
    "\n",
    "$$\n",
    "L = f_n(f_{n-1}(...f_1(x)...))\n",
    "$$\n",
    "\n",
    "Now, out of the box a neural network isn't all that useful. Its predictions will at this point will just be noise. We need to be able to train the **neural network to produce the correct output**. \n",
    "\n",
    "How do we do this?\n",
    "\n",
    "Well, what do we have control of? Our inputs? Nope. Our inputs are the data which itself can vary significantly, but we could also change our dataset entirely. We need something that sticks around, something stateful that also affects the output. \n",
    "\n",
    "What about our weights? \n",
    "\n",
    "Yes, That's exactly it! We can change our weights. They stay static unless we change them. Alright, now we have another question.\n",
    "\n",
    "How we do alter our weights?\n",
    "\n",
    "Intuitively we'd like to see what effect a weight had on the output and make a change based on that. If it played a large role in the error then we should alter it by a large amount. If it didn't play a significant role in the error, then it's probably already set to a good value and we should, for the most part, let it be.\n",
    "\n",
    "How can we formalize this idea?\n",
    "\n",
    "Luckily for us, this idea is already well established in mathematical literature where it's known as **reverse-mode differentiation** or **backpropagation**. The name sounds very scary but it's actually not so bad. It essentially boils down to computing derivatives and using the chain rule. For example, say we wanted to compute the gradient of the loss function $L$ with respect a weight $W_{jk}$, which is an input to $f_i$. Here $W$ is a matrix of weights so $W_{jk}$ is an individual weight (number) at row $j$, column $k$ of $W$.\n",
    "\n",
    "$$\n",
    "L = f_n(f_{n-1}(...f_1(x)...))\n",
    "$$\n",
    "\n",
    "Using the [chain rule](https://www.khanacademy.org/math/ap-calculus-ab/product-quotient-chain-rules-ab/chain-rule-ab/a/chain-rule-overview) the computation is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{jk}} = \n",
    "\\frac{\\partial L}{\\partial f_n}\n",
    "\\frac{\\partial f_n}{\\partial f_n}\n",
    "\\frac{\\partial f_n}{\\partial f_{n-1}}\n",
    "...\n",
    "\\frac{\\partial f_{i+1}}{\\partial f_i}\n",
    "\\frac{\\partial f_i}{\\partial W_{jk}}\n",
    "$$\n",
    "\n",
    "#### Quick Aside: Notation\n",
    "\n",
    "You may have noticed in the above link they used the following notation\n",
    "\n",
    "$$\n",
    "\\frac {d}{dx} f(g(x)) = f'(g(x))g'(x)\n",
    "$$\n",
    "\n",
    "This is equivalent to\n",
    "\n",
    "$$\n",
    "\\frac {\\partial f}{\\partial x} = \n",
    "\\frac {\\partial f}{\\partial g} \\frac {\\partial g} {\\partial x}\n",
    "$$\n",
    "\n",
    "The latter notation is used when we talk about *partial derivatives*, that is, we want measure the effect a specific input had on our output. This is precisely what we're going to be doing throughout this lab so we'll use the latter notation. By the way, the $\\partial$ symbol is called a *partial*. \n",
    "\n",
    "Ok, now back to our our friend $\\frac{\\partial L}{\\partial W_{jk}}$. This is mathematical speak for \"what effect did weight $W_{jk}$ have on the output $L$?\". Now you might be wondering why we can't just write\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{jk}} = \n",
    "\\frac{\\partial L}{\\partial f_i}\n",
    "\\frac{\\partial f_i}{\\partial W_{jk}}\n",
    "$$\n",
    "\n",
    "since $W_{jk}$ is the input to $f_i$. Why do we need to know all these other partials?. If we did that we would miss out on a wealth of information and most likely get an incorrect gradient. Remember that $f_i$ affects $f_{i+1}$ and $f_{i+1}$ affects $f_{i+2}$ and ... you get the point. Missing out on how all these intermediate function affect eachother will won't do us any good and it'll only get worse the more intermediate functions we have.\n",
    "\n",
    "I lied. We can write\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{jk}} = \n",
    "\\frac{\\partial L}{\\partial f_i}\n",
    "\\frac{\\partial f_i}{\\partial W_{jk}}\n",
    "$$\n",
    "\n",
    "but\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial f_i} =\n",
    "\\frac{\\partial L}{\\partial f_n}\n",
    "\\frac{\\partial f_n}{\\partial f_n}\n",
    "\\frac{\\partial f_n}{\\partial f_{n-1}}\n",
    "...\n",
    "\\frac{\\partial f_{i+1}}{\\partial f_i}\n",
    "$$\n",
    "\n",
    "otherwise we'll have a bad time!\n",
    "\n",
    "Ok, that's a lot of abstract thinking let's now take a look at a concrete example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study: $f = (x * y) + (x * z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\n",
    "f = * \\hspace{0.5in} f(x,y) = xy\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = y \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = x\n",
    "$$\n",
    "\n",
    "Let's think about this for a bit. What we're saying here is the change of $f$ with respect to $x$ is $y$ and vice-versa. Remember, if our derivative is with respect to $x$ we treat $y$ as a constant. So let's say $y = 10$, think about changing $x$ from 3 to 4. Then $f(3,10) = 30$ and $f(4, 10) = 40$. That's a change in 10 or $y$! Every time we change $x$ by 1, $f$ changes by $y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f = + \\hspace{0.5in} f(x,y) = x + y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 1 \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = 1\n",
    "$$\n",
    "\n",
    "Again, let's think about this. The change in $f$ with respect to $x$ is 1 and same for $y$. This means every time we change $x$ by 1, $f$ will also change by 1. This also shows $x$ and $y$ are independent of eachother.\n",
    "\n",
    "Ok, let's now use this for a simple function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: picture here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f(x, y, z) = x * y + (x * z)\n",
    "# we can split these into subexpressions\n",
    "# g(x, y) = x * y\n",
    "# h(x, z) = x * z\n",
    "# f(x, y, z) = g(x, y) + h(x, z)\n",
    "\n",
    "# intial values\n",
    "x = 3\n",
    "y = 4\n",
    "z = -5\n",
    "\n",
    "g = x * y\n",
    "h = x * z\n",
    "\n",
    "f = g + h\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take our function $f$ and apply the chain rule to compute the derivatives for $x, y, z$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial g} = 1 \\hspace{0.1in}\n",
    "\\frac{\\partial f}{\\partial h} = 1 \\hspace{0.1in}\n",
    "\\frac{\\partial g}{\\partial x} = y \\hspace{0.1in}\n",
    "\\frac{\\partial g}{\\partial y} = x \\hspace{0.1in}\n",
    "\\frac{\\partial h}{\\partial x} = z \\hspace{0.1in}\n",
    "\\frac{\\partial h}{\\partial z} = x\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = \n",
    "\\frac{\\partial f}{\\partial g}\n",
    "\\frac{\\partial g}{\\partial x}\n",
    "+\n",
    "\\frac{\\partial f}{\\partial h}\n",
    "\\frac{\\partial h}{\\partial x}\n",
    "\\hspace{0.5in}\n",
    "\\frac{\\partial f}{\\partial y} = \n",
    "\\frac{\\partial f}{\\partial g}\n",
    "\\frac{\\partial g}{\\partial y}\n",
    "\\hspace{0.5in}\n",
    "\\frac{\\partial f}{\\partial z} = \n",
    "\\frac{\\partial f}{\\partial h}\n",
    "\\frac{\\partial h}{\\partial z}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 1 * y + 1 * z = y + z\n",
    "\\hspace{0.5in}\n",
    "\\frac{\\partial f}{\\partial y} = 1 * x = x\n",
    "\\hspace{0.5in}\n",
    "\\frac{\\partial f}{\\partial z} = 1 * x = x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The above in code\n",
    "x = 3\n",
    "y = 4\n",
    "z = -5\n",
    "\n",
    "dfdg = 1.0\n",
    "dfdh = 1.0\n",
    "dgdx = y\n",
    "dgdy = x\n",
    "dhdx = z\n",
    "dhdz = x\n",
    "\n",
    "dfdx = dfdg * dgdx + dfdh * dhdx\n",
    "dfdy = dfdg * dgdy\n",
    "dfdz = dfdh * dhdz\n",
    "\n",
    "# the output of backpropagation is the gradient\n",
    "gradient = [dfdx, dfdy, dfdz]\n",
    "print(gradient) # should be [-1, 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following expression, specifically the `+` function:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = \n",
    "\\frac{\\partial f}{\\partial g}\n",
    "\\frac{\\partial g}{\\partial x}\n",
    "+\n",
    "\\frac{\\partial f}{\\partial h}\n",
    "\\frac{\\partial h}{\\partial x}\n",
    "$$\n",
    "\n",
    "Think about how $x,y,z$ flow through the graph. Both $y$ and $z$ have 1 output edge and they follow 1 path to the $f$. On the other hand, $x$ has 2 output edges and follows 2 paths to `f`. Remember, we're calculating **the derivative of f with respect to x**. In order to do this we have to consider all the ways $x$ affects $f$ (all the paths in the graph from $x$ to $f$). \n",
    "\n",
    "An easy way to see how many paths we have to consider for a node's derivative is to trace all the paths back from the output node to the input node. So if $f$ is the output node and $x$ is the input node, trace all the paths back from $f$ to $x$. It's not always the case that all the output edges of $x$ will lead to $f$, so we shouldn't just assume we have to consider all the output edges of $x$.\n",
    "\n",
    "Keep these things in mind for the node implementations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs, Nodes and Ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to draw attention to a few things from the previous section.\n",
    "\n",
    "1. The picture of the broken down expression and subexpressions of $f$ resembles a graph where the nodes are function applications.\n",
    "2. We can use of dynamic programming to make computing backpropagation efficient. Even in our simple example we see the reuse of $\\frac{\\partial f}{\\partial g}$ and $\\frac{\\partial f}{\\partial h}$. As our graph grows in size and complexity, it becomes much more evident how wasteful it is to recompute partials. The cornerstone of dynamic programming is **solving a large problem through many smaller ones** and **caching**. We'll do both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercises that you'll implement the forward and backward passes for the nodes in our graph. \n",
    "\n",
    "You'll write your code in `miniflow.py` (same directory), the autoreload extension will automatically reload your code when you make a change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from miniflow import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Implement $f = (x * y) + (x * z)$ using nodes\n",
    "\n",
    "Implement the `Mul` and `Add` nodes. The `Input` node is already provided.\n",
    "\n",
    "Take a moment to look at the `miniflow.py` file and get familiar with the node classes, don't worry too much about the other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Input()\n",
    "y = Input()\n",
    "z = Input()\n",
    "\n",
    "# TODO: implement Mul and Add in miniflow.py\n",
    "g = Mul(x, y)\n",
    "h = Mul(x, z)\n",
    "f = Add(g, h)\n",
    "\n",
    "# x, y, z nodes will take on these values and pass them to their outputs\n",
    "feed_dict = {x: 3, y: 4, z: -5}\n",
    "# compute the derivatives with respect to the following nodes\n",
    "wrt = [x, y, z]\n",
    "\n",
    "value, grad = value_and_grad(f, feed_dict, wrt)\n",
    "assert value == -3 # should be -3\n",
    "assert grad == [-1, 3, 3] # should be [-1, 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Neural Networks\n",
    "\n",
    "We are now going to take our focus on how we can use differentiable graphs to compute functions for neural networks. \n",
    "Let's assume we have a vector of features $x$, a vector of weights $w$ and a bias scalar $b$. Then we to compute output we would perform a linear transform. It's recommend you use [numpy](http://www.numpy.org/) for the following exercises.\n",
    "\n",
    "$$\n",
    "o = (\\sum_i x_iw_i) + b\n",
    "$$\n",
    "\n",
    "Or more concisely expressed as a dot product\n",
    "\n",
    "$$\n",
    "o =  x^Tw + b\n",
    "$$\n",
    "\n",
    "What if we have multiple outputs? Say we have $n$ features and $k$ outputs, then $b$ is a vector of length $k$, $x$ is a vector of length $n$ and $w$ becomes a $n$ by $k$ matrix, which we'll call $W$ from now on (matrices notation is typically a capital letter).\n",
    "\n",
    "$$\n",
    "o = x^TW + b\n",
    "$$\n",
    "\n",
    "What if we now have $m$ inputs? This is very common in practice feed in more than 1 input, it's referred to as the batch size. Then $x$ becomes a $m$ by $n$ matrix, we'll call this $X$.\n",
    "\n",
    "$$\n",
    "o = XW + b\n",
    "$$\n",
    "\n",
    "There we have it the famous linear transform! This on it's own though, is not all that powerful. We'll only do well if the data is linearably separable. This is where non-linear activations and layer stacking come into play. In fact, even a 2-layer neural network can [approximate arbitrary functions](http://neuralnetworksanddeeplearning.com/chap4.html). Pretty cool! There is however, a very fine line between being able to approximate any function theoretically and actually being able to do it efficiently and effectively in practice. If it was that easy then we wouldn't have convolution networks, recurrent neural networks, residual neural networks, generative neural network models, etc.\n",
    "\n",
    "For this lab though, we'll keep it relatively simple. By the end of the lab you'll be able to construct a train a neural network with the following architecture.\n",
    "\n",
    "$$\n",
    "Input \\rightarrow Linear \\rightarrow Sigmoid \\rightarrow Linear \\rightarrow Softmax \\rightarrow CrossEntropyLoss\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Implement `Linear` Node\n",
    "\n",
    "In this exercise we'll implement the `Linear` node. This corresponds to the following function:\n",
    "\n",
    "$$\n",
    "f(X, W, b) = XW + b\n",
    "$$\n",
    "\n",
    "There are a few ways to go about the implementation, here are some possibilities:\n",
    "\n",
    "1. Treat each element of the matrices and vector as a scalar and use existing `Mul` and `Add` nodes. You might have to implement a `Sum` node as well.\n",
    "2. Break the function up into two subfunctions, you might call these `MatMul` and `BiasAdd`.\n",
    "3. Treat it as one function.\n",
    "\n",
    "Independent of the option chosen, the `dvalues` attribute of the `Linear` node should have 3 key, value pairs. Where the value is the same size/shape as the key. Example: If $W$ is of size $nxm$ then `dvalues[W]` should also be $nxm$.\n",
    "\n",
    "**Tip: Write out the full above expression on paper and consider the derivative of $f$ with respect to a single element of $W, X, b$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_in, w_in, b_in = Input(), Input(), Input()\n",
    "# TODO: implement Linear\n",
    "f = Linear(x_in, w_in, b_in)\n",
    "\n",
    "x = np.array([[-1., -2.], [-1, -2]])\n",
    "w = np.array([[2., -3], [2., -3]])\n",
    "b = np.array([-3., -3]).reshape(1, -1)\n",
    "\n",
    "feed_dict = {x_in: x, w_in: w, b_in: b}\n",
    "loss, grad = value_and_grad(f, feed_dict, (x_in, w_in, b_in))\n",
    "assert np.allclose(loss, np.array([[-9.,  6.], [-9.,  6.]]))\n",
    "assert np.allclose(grad[0], np.array([[-1.,  -1.], [-1.,  -1.]]))\n",
    "assert np.allclose(grad[1], np.array([[-2.,  -2.], [-4., -4.]]))\n",
    "assert np.allclose(grad[2], np.array([[2., 2.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Implement `Sigmoid` Node\n",
    "\n",
    "In this exercise we'll implement the `Sigmoid` node. This corresponds to the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac {1} {1 + exp(-x)}\n",
    "$$\n",
    "\n",
    "There are a 2 ways to go about the implementation:\n",
    "\n",
    "1. Break it up into subfunctions, `Add`, `Divide`, `Exp`, etc.\n",
    "2. Try to take the derivative of the sigmoid function and see if you can simplify it. The result is suprisingly simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_in = Input()\n",
    "# TODO: implement Sigmoid\n",
    "f = Sigmoid(x_in)\n",
    "\n",
    "x = np.array([-10., 0, 10])\n",
    "feed_dict = {x_in: x}\n",
    "loss, grad = value_and_grad(f, feed_dict, [x_in])\n",
    "assert np.allclose(loss, np.array([0., 0.5, 1.]), atol=1.e-4)\n",
    "assert np.allclose(grad, np.array([0., 0.25, 0.]), atol=1.e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Implement `CrossEntropyWithSoftmax` Node\n",
    "\n",
    "In this exercise you'll implement the `CrossEntropyWithSoftmax` node. This corresponds to implementing the [softmax](https://en.wikipedia.org/wiki/Softmax_function) and [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) functions.\n",
    "\n",
    "$$\n",
    "softmax(x) = \\frac{e^{x_i}} {\\sum_j e^{x_j}}\n",
    "$$\n",
    "\n",
    "The input to the softmax function is a vector and the output is a vector of normalized probabilities. The input could also be a matrix in this case each row/example should be treated in isolation, output in this case would be a matrix of the same shape.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "a = [0.2, 1.0, 0.3]\n",
    "softmax(a) # [0.2309, 0.5138, 0.2551]\n",
    "```\n",
    "\n",
    "$$\n",
    "cross\\_entropy\\_loss(probs, labels) = \\frac {1} {n} \\sum_{i=1}^n - log(probs[i, labels_i])\n",
    "$$\n",
    "\n",
    "Where $probs$ is the output of the $softmax$ function, $labels$ are the target labels (integer values from 0 to $k-1$) and $n$ is the number of rows of $probs$. For each row $i$ we pick the element at the index corresponding with the label.\n",
    "\n",
    "Ok, but how will this loss encourage our model to classify correctly?\n",
    "\n",
    "There are 2 key pieces of information here:\n",
    "\n",
    "1. $probs$ contains values between 0 and 1\n",
    "2. $log(0) = -inf$ and $log(1) = 0$; the possible log values are either negative or 0.\n",
    "\n",
    "Clearly we'd like the probability of the index of the correct label to be 1 or close to it and if it's close to 0 we'll be heavily penalized and our will loss to shoot up. Note the negative sign sets the problem up nicely for minimizing the loss.\n",
    "\n",
    "Once again, 2 ways to about the implementation:\n",
    "\n",
    "1. Break it down into subfunctions.\n",
    "2. Similar to the sigmoid function, we can simplify the derivative. The result is even simpler than sigmoid's!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_in = Input()\n",
    "y_in = Input()\n",
    "# TODO: implement CrossEntropyWithSoftmax\n",
    "f = CrossEntropyWithSoftmax(x_in, y_in)\n",
    "\n",
    "# pretend output of a softmax\n",
    "x = np.array([[0.5, 1., 1.5]])\n",
    "y = np.array([1])\n",
    "feed_dict = {x_in: x, y_in: y}\n",
    "loss, grad = value_and_grad(f, feed_dict, wrt=[x_in])\n",
    "assert np.allclose(loss, 1.1802, atol=1.e-4)\n",
    "assert np.allclose(grad, np.array([[0.1863, -0.6928,  0.5064]]), atol=1.e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "At this point you should have all the nodes implemented and working correctly. Computing those weight gradients is a breeze! Now that we have the gradients what do we do with them?\n",
    "\n",
    "As it turns out the gradient is analogous to the **steepest ascent direction**. Let's imagine that we are our neural network and we're in a park, not just any park though, a hilly park. We also know that we perform our best when we're on the highest hill, so let's get there shall we! There are a couple of issues though ... We are 1) blindfolded and 2) dropped randomly somewhere in the park (random weights!). It's easy to see we'd be hard pressed to find the highest hill in our current condition.\n",
    "\n",
    "Luckily for us, we have a talking bird by our side, her name is Gradient and she's very good at informing us which direction causes us to ascend the hill the fastest. Unfortunately for Gradient, her vision isn't the greatest so it only applies to the local area. It might be the case we get to the top of a hill, but not the highest one.\n",
    "\n",
    "Typically we're minimizing a function, not maximizing. The above analogy still holds, we're just descending instead of ascending (searching for lowest valley vs. highest hill). Making this adjustment is very simple, just put a negative sign on the gradient.\n",
    "\n",
    "There's one more thing to think about, we know what direction to step in but how large of a step should we take? Too small a step and it could take a very long time to converge, and too large a step could overshoot the target cause divergence. In pratice the most common step sizes are 1e-3 and 1e-4, sometimes 1e-2 and 1e-1 work well too. Note: the *step size* is more commonly known as the *learning rate*.\n",
    "\n",
    "Here's roughly what SGD looks like with miniflow.\n",
    "\n",
    "```\n",
    "f = ... # output node\n",
    "wrt = [...] # weight input nodes\n",
    "W = ... # weight for Linear node\n",
    "b = ... # bias for Linear node\n",
    "while True:\n",
    "    feed_dict = {...} # new batch of data\n",
    "    loss, grad = value_and_grad(f, feed_dict, wrt)\n",
    "    # get the grad for W and b\n",
    "    dW = grad[...]\n",
    "    db = grad[...] \n",
    "    # descending in the direction of the gradient\n",
    "    W[:] -= step_size * dW\n",
    "    b[:] -= step_size * db\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Train a neural net on MNIST with SGD\n",
    "\n",
    "You now have all the pieces in place, all that's left to do is fit them together like legos.\n",
    "\n",
    "TODO: lego picture\n",
    "\n",
    "Your task is now to classify handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load MNIST\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getting input and label data\n",
    "X_batch, y_batch = mnist.train.next_batch(128)\n",
    "print(X_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_val = mnist.validation.images\n",
    "y_val = mnist.validation.labels.astype(np.int8)\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels.astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Train a neural net on MNIST with SGD\n",
    "\n",
    "You should hit above 90% accuracy on even a 1-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: initialize weights\n",
    "\n",
    "# TODO: define architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: set step size and batch size\n",
    "step_size = ...\n",
    "batch_size = ...\n",
    "for i in range(10000):\n",
    "    X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "    y_batch = y_batch.astype(np.int8)\n",
    "    # TODO: get the loss and grad\n",
    "    loss, grad = value_and_grad(...)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\"Iteration {}\".format(i))\n",
    "        print(\"Loss = {}\".format(loss))\n",
    "        # TODO: get the accuracy of the validation dataset\n",
    "        # similar to value_and_grad except there's no wrt list\n",
    "        acc = accuracy(...)\n",
    "        print(\"Validation Accuracy = {}\".format(acc))\n",
    "        print('----------------')\n",
    "        \n",
    "    # TODO: SGD updates with gradient\n",
    "   \n",
    "    \n",
    "# TODO: get the accuracy of the testing dataset\n",
    "acc = accuracy(...)\n",
    "print('----------------')\n",
    "print(\"Testing Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outro to TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you created a library, miniflow, for computing differentiable graphs. Using miniflow, you created a neural network that can classiify handwritten digits with high accuracy. Not too shabby! Hopefully you now have a solid grasp of differentiable graphs and backpropagation. \n",
    "\n",
    "You probably also noticed how tedious all this work can be! It would be nice if someone already had written all this code for us, made it really fast, usable on GPUs, mobile devices and had a gigantic ecosystem around it so we can be assured it'll just get better and better ... Oh wait! It's not a fantasy. All of that is true and it's called [Tensorflow](https://www.tensorflow.org/), developed by the fine folks at Google and now a thriving open source project! From now on we'll use TensorFlow, or miniflow if you prefer, since you've grown so attached ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
