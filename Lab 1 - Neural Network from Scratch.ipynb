{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "\n",
    "In this lab we'll create a neural network from scratch. In the process we'll learn about the algorithm that makes it all possible, **backpropagation**. In the end we'll end up with a mini Tensorflow-like library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "1. Intro to Backprop + chain rule, derivatives of (+, * , -) topped with a simple example\n",
    "2. Transition to thinking about functions as nodes with inputs and outputs in a graph. Rewrite previous example in these terms.\n",
    "3. Matrix/vector, sigmoid ops + derivatives. MSE or Cross-entropy depending on if weâ€™re regressing or classifying. \n",
    "4. Create neural net + train with SGD (MNIST?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation and the Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Improve this intro and change the exercise to $f = (x * y) + (x * -z)$ this illustrates what happens when a node has multiple outputs.\n",
    "\n",
    "The fundamental idea behind neural networks is to minimize an objective, typically known as the *loss function*. The loss function outputs a single number that tells us how well we're doing, the smaller the better.\n",
    "\n",
    "We can think of a neural network as a composition of functions with a loss (number) as the output.\n",
    "\n",
    "$$L = f_n(f_{n-1}(...f_1(x)...))$$\n",
    "\n",
    "The idea behind backpropagation, also known as reverse-mode differentiation is to use the chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial f_n}\n",
    "\\frac{\\partial f_n}{\\partial f_{n-1}}\n",
    "...\n",
    "\\frac{\\partial f_1}{\\partial f_x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: transition from above abstract functions to things like multplication and addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\n",
    "f = * \\hspace{0.5in} f(x,y) = xy\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = y \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = x\n",
    "$$\n",
    "\n",
    "Let's think about this for a bit. What we're saying here is the change of $f$ with respect to $x$ is $y$ and vice-versa. Remember, if our derivative is with respect to $x$ we treat $y$ as a constant. So let's say $y = 10$, think about changing $x$ from 3 to 4. Then $f(3,10) = 30$ and $f(4, 10) = 40$. That's a change in 10 or $y$! Every time we change $x$ by 1, $f$ changes by $y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f = + \\hspace{0.5in} f(x,y) = x + y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 1 \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = 1\n",
    "$$\n",
    "\n",
    "Again, let's think about this. The change in $f$ with respect to $x$ is 1 and same for $y$. This means every time we change $x$ by 1, $f$ will also change by 1. This also shows $x$ and $y$ are independent of eachother.\n",
    "\n",
    "Ok, let's now use this for a simple function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: picture here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f(x, y, z) = x * y + (x * z)\n",
    "# we can split these into subexpressions\n",
    "# g(x, y) = x * y\n",
    "# h(x, z) = x * z\n",
    "# f(x, y, z) = g(x, y) + h(x, z)\n",
    "\n",
    "# intial values\n",
    "x = 3\n",
    "y = 4\n",
    "z = -5\n",
    "\n",
    "g = x * y\n",
    "h = x * z\n",
    "\n",
    "f = g + h\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take our function $f$ and apply the chain rule to compute the derivatives for $x, y, z$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial g} = 1 \\hspace{0.1in}\n",
    "\\frac{\\partial f}{\\partial h} = 1 \\hspace{0.1in}\n",
    "\\frac{\\partial g}{\\partial x} = y \\hspace{0.1in}\n",
    "\\frac{\\partial g}{\\partial y} = x \\hspace{0.1in}\n",
    "\\frac{\\partial h}{\\partial x} = z \\hspace{0.1in}\n",
    "\\frac{\\partial h}{\\partial z} = x\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = \n",
    "\\frac{\\partial f}{\\partial g}\n",
    "\\frac{\\partial g}{\\partial x}\n",
    "+\n",
    "\\frac{\\partial f}{\\partial h}\n",
    "\\frac{\\partial h}{\\partial x}\n",
    "\\hspace{0.5in}\n",
    "\\frac{\\partial f}{\\partial y} = \n",
    "\\frac{\\partial f}{\\partial g}\n",
    "\\frac{\\partial g}{\\partial y}\n",
    "\\hspace{0.5in}\n",
    "\\frac{\\partial f}{\\partial z} = \n",
    "\\frac{\\partial f}{\\partial h}\n",
    "\\frac{\\partial h}{\\partial z}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 1 * y + 1 * z = y * z\n",
    "\\hspace{0.5in}\n",
    "\\frac{\\partial f}{\\partial y} = 1 * x = x\n",
    "\\hspace{0.5in}\n",
    "\\frac{\\partial f}{\\partial z} = 1 * x = x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The above in code\n",
    "x = 3\n",
    "y = 4\n",
    "z = -5\n",
    "\n",
    "dfdg = 1.0\n",
    "dfdh = 1.0\n",
    "dgdx = y\n",
    "dgdy = x\n",
    "dhdx = z\n",
    "dhdz = x\n",
    "\n",
    "dfdx = dfdg * dgdx + dfdh * dhdx\n",
    "dfdy = dfdg * dgdy\n",
    "dfdz = dfdh * dhdz\n",
    "\n",
    "# the output of backpropagation is the gradient\n",
    "gradient = [dfdx, dfdy, dfdz]\n",
    "print(gradient) # [-1, 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following expression, specifically the `+` function:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = \n",
    "\\frac{\\partial f}{\\partial g}\n",
    "\\frac{\\partial g}{\\partial x}\n",
    "+\n",
    "\\frac{\\partial f}{\\partial h}\n",
    "\\frac{\\partial h}{\\partial x}\n",
    "$$\n",
    "\n",
    "Think about how $x,y,z$ flow through the graph. Both $y$ and $z$ have 1 output edge and they follow 1 path to the $f$. On the other hand, $x$ has 2 output edges and follows 2 paths to `f`. Remember, we're calculating **the derivative of f with respect to x**. In order to do this we have to consider all the ways $x$ affects $f$ (all the paths in the graph from $x$ to $f$). \n",
    "\n",
    "An easy way to see how many paths we have to consider for a node's derivative is to trace all the paths back from the output node to the input node. So if $f$ is the output node and $x$ is the input node, trace all the paths back from $f$ to $x$. It's not always the case that all the output edges of $x$ will lead to $f$, so we shouldn't just assume we have to consider all the output edges of $x$.\n",
    "\n",
    "Keep these things in mind for the node implementations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs, Nodes and Ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to draw attention to a few things from the previous section.\n",
    "\n",
    "1. The picture of the broken down expression and subexpressions of $f$ resembles a graph where the nodes are function applications.\n",
    "2. We can use of dynamic programming to make computing backpropagation efficient. Even in our simple example we see the reuse of $\\frac{\\partial f}{\\partial g}$ and $\\frac{\\partial f}{\\partial h}$. As our graph grows in size and complexity, it becomes much more evident how wasteful it is to recompute partials. The cornerstone of dynamic programming is **solving a large problem through many smaller ones** and **caching**. We'll do both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercises that you'll implement the forward and backward passes for the nodes in our graph. \n",
    "\n",
    "You'll write your code in `miniflow.py` (same directory), the autoreload extension will automatically reload your code when you make a change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from miniflow import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Implement $f = (x * y) + (x * z)$ using nodes\n",
    "\n",
    "Implement the `Mul` and `Add` nodes. The `Input` node is already provided.\n",
    "\n",
    "Node template to get started\n",
    "\n",
    "```\n",
    "class Node(object):\n",
    "    def __init__(self, input_nodes=[]):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = []\n",
    "        self.cache = {}\n",
    "        self.value = 0\n",
    "        self.dvalues = {}\n",
    "\n",
    "    def forward(self):\n",
    "        # TODO: implement and store in self.value\n",
    "        \n",
    "    def backward(self):\n",
    "        # TODO: implement and store in self.dvalues\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = Input()\n",
    "y = Input()\n",
    "z = Input()\n",
    "\n",
    "# TODO: implement Mul and Add in miniflow.py\n",
    "g = Mul(x, y)\n",
    "h = Mul(x, z)\n",
    "f = Add(g, h)\n",
    "\n",
    "# x, y, z nodes will take on these values and pass them to their outputs\n",
    "feed_dict = {x: 3, y: 4, z: -5}\n",
    "# compute the derivatives with respect to the following nodes\n",
    "wrt = [x, y, z]\n",
    "\n",
    "value, grad = value_and_grad(f, feed_dict, wrt)\n",
    "assert value == -3 # should be -3\n",
    "assert grad == [-1, 3, 3] # should be [-1, 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Neural Networks\n",
    "\n",
    "Content: Introduce Linear, Sigmoid and CrossEntropyLoss nodes. Matrices, vectors, activation functions, loss functions.\n",
    "\n",
    "We're now going to take our focus on how we can use differentiable graphs to compute functions for neural networks. \n",
    "Let's assume we have a vector of features $x$, a vector of weights $w$ and a bias scalar $b$. Then we to compute output we would perform a linear transform.\n",
    "\n",
    "$$\n",
    "o = (\\sum_i x_iw_i) + b\n",
    "$$\n",
    "\n",
    "Or more concisely expressed as a dot product\n",
    "\n",
    "$$\n",
    "o =  x^Tw + b\n",
    "$$\n",
    "\n",
    "What if we have multiple outputs? Say we have $n$ features and $k$ outputs, then $b$ is a vector of length $k$, $x$ is a vector of length $n$ and $w$ becomes a $n$ by $k$ matrix, which we'll call $W$ from now on (matrices notation is typically a capital letter).\n",
    "\n",
    "$$\n",
    "o = x^TW + b\n",
    "$$\n",
    "\n",
    "What if we now have $m$ inputs? This is very common in practice feed in more than 1 input, it's referred to as the batch size. Then $x$ becomes a $m$ by $n$ matrix, we'll call this $X$.\n",
    "\n",
    "$$\n",
    "o = XW + b\n",
    "$$\n",
    "\n",
    "There we have it the famous linear transform! This on it's own though, is not all that powerful. We'll only do well if the data is linearably separable. This is where non-linear activations and layer stacking come into play. In fact, even a 2-layer neural network can [approximate arbitrary functions](http://neuralnetworksanddeeplearning.com/chap4.html). Pretty cool! There is however, a very fine line between being able to approximate any function theoretically and actually being able to do it efficiently and effectively in practice. If it was that easy then we wouldn't have convolution networks, recurrent neural networks, residual neural networks, generative neural network models, etc.\n",
    "\n",
    "For this lab though, we'll keep it relatively simple. By the end of the lab you'll be able to construct a train a neural network with the following architecture.\n",
    "\n",
    "$$\n",
    "Input \\rightarrow Linear \\rightarrow Sigmoid \\rightarrow Linear \\rightarrow Softmax \\rightarrow CrossEntropyLoss\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Implement `Linear` Node\n",
    "\n",
    "In this exercise we'll implement the `Linear` node. This corresponds to the following function:\n",
    "\n",
    "$$\n",
    "f(X, W, b) = XW + b\n",
    "$$\n",
    "\n",
    "There are a few ways to go about the implementation, here are some possibilities:\n",
    "\n",
    "1. Treat each element of the matrices and vector as a scalar and use existing `Mul` and `Add` nodes. You might have to implement a `Sum` node as well.\n",
    "2. Break the function up into two subfunctions, you might call these `MatMul` and `BiasAdd`.\n",
    "3. Treat it as one function.\n",
    "\n",
    "Independent of the option chosen, the `dvalues` attribute of the `Linear` node should have 3 key, value pairs. Where the value is the same size/shape as the key. Example: If $W$ is of size $nxm$ then `dvalues[W]` should also be $nxm$.\n",
    "\n",
    "**Tip: Write out the full above expression on paper and consider the derivative of $f$ with respect to a single element of $W, X, b$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_in, w_in, b_in = Input(), Input(), Input()\n",
    "# TODO: implement Linear\n",
    "f = Linear(x_in, w_in, b_in)\n",
    "\n",
    "x = np.array([[-1., -2.], [-1, -2]])\n",
    "w = np.array([[2., -3], [2., -3]])\n",
    "b = np.array([-3., -3]).reshape(1, -1)\n",
    "\n",
    "feed_dict = {x_in: x, w_in: w, b_in: b}\n",
    "loss, grad = value_and_grad(f, feed_dict, (x_in, w_in, b_in))\n",
    "assert np.allclose(loss, np.array([[-9.,  6.], [-9.,  6.]]))\n",
    "assert np.allclose(grad[0], np.array([[-1.,  -1.], [-1.,  -1.]]))\n",
    "assert np.allclose(grad[1], np.array([[-2.,  -2.], [-4., -4.]]))\n",
    "assert np.allclose(grad[2], np.array([[2., 2.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Implement `Sigmoid` Node\n",
    "\n",
    "In this exercise we'll implement the `Sigmoid` node. This corresponds to the following function:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac {1} {1 + exp(-x)}\n",
    "$$\n",
    "\n",
    "Where $x$ is the output of a `Linear` node.\n",
    "\n",
    "There are a 2 ways to go about the implementation:\n",
    "\n",
    "1. Break it up into subfunctions, `Add`, `Divide`, `Exp`, etc.\n",
    "2. Try to take the derivative of the sigmoid function and see if you can simplify it. The result is suprisingly simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_in = Input()\n",
    "# TODO: implement Sigmoid\n",
    "f = Sigmoid(x_in)\n",
    "\n",
    "x = np.array([-10., 0, 10])\n",
    "feed_dict = {x_in: x}\n",
    "loss, grad = value_and_grad(f, feed_dict, [x_in])\n",
    "assert np.allclose(loss, np.array([0., 0.5, 1.]), atol=1.e-4)\n",
    "assert np.allclose(grad, np.array([0., 0.25, 0.]), atol=1.e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Implement `CrossEntropyLoss` Node\n",
    "\n",
    "In this exercise we'll implement the `CrossEntropyLoss` node. This corresponds to the following functions:\n",
    "\n",
    "$$\n",
    "softmax(x_i) = \\frac{e^{x_i}} {\\sum_j e^{x_j}}\n",
    "$$\n",
    "\n",
    "The input to the $softmax$ function should be a $n x k$ matrix, where $n$ is the number of examples (batch size) and $k$ is the number classes an example can belong to. The $softmax$ function then computes probabilities (likelihood) of the example belonging to each class. In this case $\\sum_j e^{x_j}$ sums over a row in the matrix and $e^{x_i}$ is a singular element in that row. The output of the $softmax$ should be a matrix (same shape as the input) of probabilities.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "a = [0.2, 1.0, 0.3]\n",
    "softmax(a) # [0.2309, 0.5138, 0.2551]\n",
    "```\n",
    "\n",
    "$$\n",
    "cross\\_entropy\\_loss(probs, labels) = \\frac{\\sum_i -log(probs_{labels_i})} {n}\n",
    "$$\n",
    "\n",
    "Where $probs$ is the output of the $softmax$ function, $labels$ are the target labels (integer values from 0 to $k-1$) and $n$ is the number of rows of $probs$. For each row $i$ we pick the element at the index corresponding with the label. \n",
    "\n",
    "Ok, but how will this loss encourage our model to classify correctly?\n",
    "\n",
    "There are 2 key pieces of information here:\n",
    "\n",
    "1. $probs$ contains values between 0 and 1\n",
    "2. $log(0) = -inf$ and $log(1) = 0$\n",
    "\n",
    "TODO: log function graph?\n",
    "\n",
    "Clearly we want the probability of the index of the correct label to be 1 or close to it and if it's close to 0 we'll be heavily penalized and our loss to shoot up. \n",
    "\n",
    "The negative sign in front of the log is simply so our objective is to minimize instead of maximize. The values produced by log between 0 and 1 will be negative or 0.\n",
    "\n",
    "Once again, 2 ways to about the implementation:\n",
    "\n",
    "1. Break it down into subfunctions.\n",
    "2. Similar to the sigmoid function, we can simplify the derivative. The result is even simpler than sigmoid's!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_in = Input()\n",
    "y_in = Input()\n",
    "# TODO: implement CrossEntropyLoss\n",
    "f = CrossEntropyLoss(x_in, y_in)\n",
    "\n",
    "# pretend output of a softmax\n",
    "x = np.array([[0.5, 1., 1.5]])\n",
    "y = np.array([1])\n",
    "feed_dict = {x_in: x, y_in: y}\n",
    "loss, grad = value_and_grad(f, feed_dict, wrt=[x_in])\n",
    "assert np.allclose(loss, 1.1802, atol=1.e-4)\n",
    "assert np.allclose(grad, np.array([[0.1863, -0.6928,  0.5064]]), atol=1.e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create neural net to train MNIST\n",
    "\n",
    "Content: neural network layers, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all our pieces in place, all that's left to do is stack them together like legos.\n",
    "\n",
    "TODO: lego picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Make a 2-layer neural net to train MNIST using `Input`, `Linear`, `Sigmoid`, `CrossEntropyLoss` nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Training with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just here for a small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "for j in range(K):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.linspace(0.0,1,N) # radius\n",
    "    t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    y[ix] = j\n",
    "# lets visualize the data:\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_in, W_in, b_in, y_in = Input(), Input(), Input(), Input()\n",
    "f = Linear(X_in, W_in, b_in)\n",
    "f = CrossEntropyLoss(f, y_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# works\n",
    "alpha = 1e-0\n",
    "for i in range(200):\n",
    "    feed_dict = {X_in: X, W_in: W, b_in: b, y_in: y}\n",
    "    loss, grad = value_and_grad(f, feed_dict, [W_in, b_in])\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"Iteration {}: Loss = {}\".format(i, loss))\n",
    "        \n",
    "    # SGD\n",
    "    dW, db = grad\n",
    "    W -= alpha * dW\n",
    "    b -= alpha * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
